{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Gender recognition by voice\n",
    "## EPFL - Statistical learning (MATH-412) \n",
    "## Adrien Besson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. An intuitive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.discriminant_analysis as lda\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn import svm, tree, ensemble, neural_network, mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "input_file = os.path.join(os.getcwd(), 'data', 'voice.csv')\n",
    "data = pd.read_csv(input_file)\n",
    "data['label'] = data['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop collinear columns\n",
    "cols_to_drop = ['IQR', 'dfrange', 'centroid']\n",
    "data = data.drop(cols_to_drop,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign quantitative values to the labels and drop them from the data\n",
    "encoder = preproc.LabelEncoder()\n",
    "labels = data['label'].values\n",
    "labels = encoder.fit_transform(labels)\n",
    "data = data.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification based on the mean frequency / mean fundamental frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our intuitive idea is to consider the mean fequency / mean fundamental frequency as a good classifier between male and female. Indeed, it is clear that male's voice have lower frequencies than female's one. Let's try this idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "features = data['meanfun'].values.reshape(-1,1)\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, labels, train_size=0.8, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA/QDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss LDA: 4.889589905362779 %\n"
     ]
    }
   ],
   "source": [
    "lda_class = lda.LinearDiscriminantAnalysis()\n",
    "lda_class.fit(X=features_train, y=labels_train)\n",
    "class_score_lda = lda_class.score(X=features_test, y=labels_test)\n",
    "print('Classification loss LDA: {0} %'.format((1-class_score_lda)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss QDA: 4.889589905362779 %\n"
     ]
    }
   ],
   "source": [
    "qda_class = lda.QuadraticDiscriminantAnalysis()\n",
    "qda_class.fit(X=features_train, y=labels_train)\n",
    "class_score_qda = qda_class.score(X=features_test, y=labels_test)\n",
    "print('Classification loss QDA: {0} %'.format((1-class_score_qda)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression - Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss Logistic - Ridge: 4.731861198738175\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression with L2 regularization\n",
    "list_C = np.logspace(10^-3, 10^3, 200)\n",
    "logistic_reg = lm.LogisticRegression(penalty='l2', solver='liblinear', random_state=10)\n",
    "max_score = 0\n",
    "for C in list_C:\n",
    "    logistic_reg.set_params(C=C)\n",
    "    logistic_reg.fit(features_train, labels_train)\n",
    "    score = logistic_reg.score(features_test, labels_test)\n",
    "    if score > max_score:\n",
    "        best_C = C\n",
    "        max_score = score\n",
    "\n",
    "logistic_reg.set_params(C=best_C)\n",
    "logistic_reg.fit(X=features_train, y=labels_train)\n",
    "class_score_reg_l2 = logistic_reg.score(X=features_test, y=labels_test)\n",
    "print('Classification loss Logistic - Ridge: {0}'.format((1-class_score_reg_l2)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression - LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss Logistic - LASSO: 4.889589905362779\n"
     ]
    }
   ],
   "source": [
    "list_C = np.logspace(10^-3, 10^3, 200)\n",
    "logistic_reg_l1 = lm.LogisticRegression(penalty='l1', solver='liblinear', random_state=10)\n",
    "max_score = 0\n",
    "best_C = list_C[0]\n",
    "for C in list_C:\n",
    "    logistic_reg_l1.set_params(C=C)\n",
    "    logistic_reg_l1.fit(features_train, labels_train)\n",
    "    score = logistic_reg_l1.score(features_test, labels_test)\n",
    "    if score > max_score:\n",
    "        best_C = C\n",
    "        max_score = score\n",
    "        \n",
    "logistic_reg_l1.set_params(C=best_C)\n",
    "logistic_reg_l1.fit(X=features_train, y=labels_train)\n",
    "class_score_reg_l1 = logistic_reg_l1.score(X=features_test, y=labels_test)\n",
    "print('Classification loss Logistic - LASSO: {0}'.format((1-class_score_reg_l1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss linear SVM - L2: 4.731861198738175 %\n"
     ]
    }
   ],
   "source": [
    "# SVM classification - Linear kernel\n",
    "list_C = np.logspace(10^-3, 10^3, 100)\n",
    "class_svm = svm.LinearSVC(random_state=10)\n",
    "max_score = 0\n",
    "best_C = list_C[0]\n",
    "for C in list_C:\n",
    "    class_svm.set_params(C=C)\n",
    "    class_svm.fit(features_train, labels_train)\n",
    "    score = class_svm.score(features_test, labels_test)\n",
    "    if score > max_score:\n",
    "        best_C = C\n",
    "        max_score = score\n",
    "class_svm.set_params(C=best_C)\n",
    "class_svm.fit(X=features_train, y=labels_train)\n",
    "score_svm = class_svm.score(X=features_test, y=labels_test)\n",
    "print('Classification loss linear SVM - L2: {0} %'.format((1-score_svm)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM - RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss kernel SVM - L2: 4.731861198738175 %\n"
     ]
    }
   ],
   "source": [
    "# SVM classification - Linear kernel\n",
    "list_C = np.logspace(10^-3, 10^3, 100)\n",
    "class_svm = svm.SVC(kernel='rbf',random_state=10)\n",
    "max_score = 0\n",
    "best_C = list_C[0]\n",
    "for C in list_C:\n",
    "    class_svm.set_params(C=C)\n",
    "    class_svm.fit(features_train, labels_train)\n",
    "    score = class_svm.score(features_test, labels_test)\n",
    "    if score > max_score:\n",
    "        best_C = C\n",
    "        max_score = score\n",
    "class_svm.set_params(C=best_C)\n",
    "class_svm.fit(X=features_train, y=labels_train)\n",
    "score_svm = class_svm.score(X=features_test, y=labels_test)\n",
    "print('Classification loss kernel SVM - L2: {0} %'.format((1-score_svm)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Decision tree: 5.678233438485803 %\n"
     ]
    }
   ],
   "source": [
    "dec_tree = tree.DecisionTreeClassifier(criterion='gini', random_state=10)\n",
    "dec_tree.fit(X=features_train, y=labels_train)\n",
    "score_tree = dec_tree.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Decision tree: {0} %'.format((1-score_tree)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Random forest: 5.835962145110408 %\n"
     ]
    }
   ],
   "source": [
    "random_forest = ensemble.RandomForestClassifier(criterion='gini', random_state=10)\n",
    "random_forest.fit(X=features_train, y=labels_train)\n",
    "score_rf = random_forest.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Random forest: {0} %'.format((1-score_rf)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - AdaBoost: 5.047318611987384 %\n"
     ]
    }
   ],
   "source": [
    "ada_boost = ensemble.AdaBoostClassifier(random_state=10)\n",
    "ada_boost.fit(X=features_train, y=labels_train)\n",
    "score_ab = ada_boost.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - AdaBoost: {0} %'.format((1-score_ab)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Gradient Boosting: 5.3627760252365935 %\n"
     ]
    }
   ],
   "source": [
    "g_boost = ensemble.GradientBoostingClassifier(random_state=10)\n",
    "g_boost.fit(X=features_train, y=labels_train)\n",
    "score_gb = g_boost.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Gradient Boosting: {0} %'.format((1-score_gb)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Bagging: 5.835962145110408 %\n"
     ]
    }
   ],
   "source": [
    "bagging = ensemble.BaggingClassifier(random_state=10)\n",
    "bagging.fit(X=features_train, y=labels_train)\n",
    "score_bag = bagging.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Bagging: {0} %'.format((1-score_bag)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "1. The mean frequency leads to a relatively low classification loss, around 35%. It is a good feature for classification. This makes sense due to the difference of the voice frequencies between men and women;\n",
    "1. The mean fundamental frequency leads to a very low classification loss, around 5%. The underlying physical reason may be that the fundamental frequency of the male's voice cannot be reached by female voice. This is confirmed by publications studies (https://hal.archives-ouvertes.fr/halshs-00999332/document)\n",
    "1. The classification loss is similar for all the classifier. It is not obvious to identify a best classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
