{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Gender recognition by voice\n",
    "## EPFL - Statistical learning (MATH-412) \n",
    "## Adrien Besson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. An intuitive approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.discriminant_analysis as lda\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn import svm, tree, ensemble, neighbors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "input_file = os.path.join(os.getcwd(), 'data', 'voice.csv')\n",
    "data = pd.read_csv(input_file)\n",
    "data['label'] = data['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop collinear columns\n",
    "cols_to_drop = ['IQR', 'dfrange', 'centroid']\n",
    "data = data.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign quantitative values to the labels and drop them from the data\n",
    "encoder = preproc.LabelEncoder()\n",
    "labels = data['label'].values\n",
    "labels = encoder.fit_transform(labels)\n",
    "data = data.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification based on the fundamental frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our intuitive idea is to consider the mean fundamental frequency as a good classifier between male and female. Indeed, most state-of-the-art studies on automatic gender recognition state that the fundamental frequency is a good classifier. Let's try this idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "features = data['meanfun'].values.reshape(-1,1)\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, labels, train_size=0.8, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss kNN: 5.205047318611989 %\n"
     ]
    }
   ],
   "source": [
    "# k-NN classification\n",
    "param_grid = [\n",
    "  {'n_neighbors': np.arange(1, 50, 1)}\n",
    "]\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn_grid = model_selection.GridSearchCV(knn, param_grid)\n",
    "knn_grid.fit(X=features_train, y=labels_train)\n",
    "class_score_knn = knn_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss kNN: {0} %'.format((1-class_score_knn)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA/QDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - LDA: 4.889589905362779 %\n"
     ]
    }
   ],
   "source": [
    "lda_class = lda.LinearDiscriminantAnalysis()\n",
    "lda_class.fit(X=features_train, y=labels_train)\n",
    "class_score_lda = lda_class.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - LDA: {0} %'.format((1-class_score_lda)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - QDA: 4.889589905362779 %\n"
     ]
    }
   ],
   "source": [
    "qda_class = lda.QuadraticDiscriminantAnalysis()\n",
    "qda_class.fit(X=features_train, y=labels_train)\n",
    "class_score_qda = qda_class.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - QDA: {0} %'.format((1-class_score_qda)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Logistic : 5.3627760252365935 %\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "logistic = lm.LogisticRegression(penalty='l2', solver='liblinear', random_state=10)\n",
    "logistic.set_params(C=1e9)\n",
    "logistic.fit(features_train, labels_train)\n",
    "score = logistic.score(features_test, labels_test)\n",
    "logistic.fit(X=features_train, y=labels_train)\n",
    "class_score_l2 = logistic.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Logistic : {0} %'.format((1-class_score_l2)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression - Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Logistic Ridge: 5.047318611987384 %\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression with L2 regularization\n",
    "list_C = np.logspace(10^-3, 10^3, 200)\n",
    "logistic_reg = lm.LogisticRegressionCV(penalty='l2', solver='liblinear', random_state=10)\n",
    "logistic_reg.fit(X=features_train, y=labels_train)\n",
    "class_score_reg_l2 = logistic_reg.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Logistic Ridge: {0} %'.format((1-class_score_reg_l2)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression - LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Logistic LASSO: 5.3627760252365935\n"
     ]
    }
   ],
   "source": [
    "logistic_reg_l1 = lm.LogisticRegressionCV(penalty='l1', solver='liblinear', random_state=10)\n",
    "logistic_reg_l1.fit(X=features_train, y=labels_train)\n",
    "class_score_reg_l1 = logistic_reg_l1.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Logistic LASSO: {0}'.format((1-class_score_reg_l1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - linear SVM: 5.047318611987384 %\n"
     ]
    }
   ],
   "source": [
    "# SVM classification - Linear kernel\n",
    "param_grid = [\n",
    "  {'C': np.logspace(10^-3, 10^3, 100)}\n",
    "]\n",
    "class_svm = svm.LinearSVC(random_state=10)\n",
    "class_svm_grid = model_selection.GridSearchCV(class_svm, param_grid)\n",
    "class_svm_grid.fit(X=features_train, y=labels_train)\n",
    "score_svm = class_svm_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - linear SVM: {0} %'.format((1-score_svm)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM - RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - kernel SVM - L2: 4.889589905362779 %\n"
     ]
    }
   ],
   "source": [
    "# SVM classification - RBF kernel\n",
    "param_grid = [\n",
    "  {'C': np.logspace(10^-3, 10^3, 100)}\n",
    "]\n",
    "class_svm = svm.SVC(kernel='rbf',random_state=10)\n",
    "class_svm_grid = model_selection.GridSearchCV(class_svm, param_grid)\n",
    "class_svm_grid.fit(X=features_train, y=labels_train)\n",
    "score_svm = class_svm_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - kernel SVM - L2: {0} %'.format((1-score_svm)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Decision tree: 5.047318611987384 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "  {'min_samples_leaf': [1, 5,10,50,100,200,500]}\n",
    "]\n",
    "dec_tree = tree.DecisionTreeClassifier(criterion='entropy', random_state=10)\n",
    "dec_tree_grid = model_selection.GridSearchCV(dec_tree, param_grid)\n",
    "dec_tree_grid.fit(X=features_train, y=labels_train)\n",
    "score_tree = dec_tree_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Decision tree: {0} %'.format((1-score_tree)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Random forest: 5.205047318611989 %\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "  {'min_samples_leaf': [1, 5,10,50,100,200,500]}\n",
    "]\n",
    "random_forest = ensemble.RandomForestClassifier(criterion='entropy', random_state=10, n_jobs=-1)\n",
    "random_forest_grid = model_selection.GridSearchCV(random_forest, param_grid)\n",
    "random_forest_grid.fit(X=features_train, y=labels_train)\n",
    "score_rf = random_forest_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Random forest: {0} %'.format((1-score_rf)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - AdaBoost: 5.047318611987384 %\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "  {'n_estimators': [1, 5,10,50,100,200,500], 'learning_rate':[1e-3, 1e-2, 1e-1, 1, 10]}\n",
    "]\n",
    "ada_boost = ensemble.AdaBoostClassifier(random_state=10)\n",
    "ada_boost_grid = model_selection.GridSearchCV(ada_boost, param_grid)\n",
    "ada_boost_grid.fit(X=features_train, y=labels_train)\n",
    "score_ab = ada_boost_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - AdaBoost: {0} %'.format((1-score_ab)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Gradient Boosting: 5.047318611987384 %\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "  {'n_estimators': [1, 5,10,50,100,200,500], 'learning_rate':[1e-3, 1e-2, 1e-1, 1, 10], 'min_samples_leaf': [1, 5,10,50,100,200,500]}\n",
    "]\n",
    "g_boost = ensemble.GradientBoostingClassifier(random_state=10)\n",
    "g_boost_grid = model_selection.GridSearchCV(g_boost, param_grid)\n",
    "g_boost_grid.fit(X=features_train, y=labels_train)\n",
    "score_gb = g_boost_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Gradient Boosting: {0} %'.format((1-score_gb)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Bagging: 5.047318611987384 %\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "  {'n_estimators': [1, 5,10,50,100,200,500]}\n",
    "]\n",
    "bagging = ensemble.BaggingClassifier(random_state=10)\n",
    "bagging_grid = model_selection.GridSearchCV(bagging, param_grid)\n",
    "bagging_grid.fit(X=features_train, y=labels_train)\n",
    "score_bag = bagging_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Bagging: {0} %'.format((1-score_bag)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
