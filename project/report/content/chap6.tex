\chapter{Conclusion}
\label{chap_conclusion}

In this work, we are interested in the problem of automatic gender recognition from voice. Given a dataset of spectral features extracted from \num{3168} labelled voice recordings, the objective is to design the most efficient gender classifier.

We compare an extensive number of state-of-the-art methods, \ie{} logistic regression, regularized logistic regression, linear discriminant analysis, quadratic discriminant analysis, k-nearest neighbors, decision tree, random forests, bagging, gradient boosting trees~(XGBoost), linear and kernel-based~(Gaussian) support vector machines. To perform the comparison, we achieve a $50/50$ split of our dataset. The first subset is used for best parameter selection and the second subset for model comparison based on a \num{5}-fold averaging of the test error to reduce the variance. We show that all the methods perform very well, with an average classification error of \SI{2,98}{\percent} and demonstrate that XGBoost gives the lowest classification error, with an average of \SI{2,01}{\percent}.
We then perform a preliminary test of its robustness against small amount of noise, based on our own dataset of \num{11} voice recordings, acquired in a crowded place of EPFL. We show that XGBoost is robust to small amount of noise since it classifies well all the voices.

In addition to this comparison, we perform an in-depth study of the features involved in the classification since we believe that it is interesting to see which spectral information is most relevant for gender recognition. It appears that the most important feature is by far the mean fundamental frequency, which translates physiological differences between males and females, \eg{} glottal shape and size of the vocal chords. Indeed, with this feature alone, the average classification error of the different models is already~\SI{5,05}{\percent}. Apart from it, statistical spectral features such as the first and third frequency quartile, the minimum and maximum fundamental frequencies are also important. More interestingly, the flatness of the spectrum seems to import more than the asymmetry or the kurtosis. 

Thus, we manage to both identify the best classifier by establishing a rigorous comparison process and to understand the high importance of some features by performing an in-depth data analysis. In order to provide a really efficient method for real-word applications, the next step is to perform an in-depth study of the robustness of the classifier against many sources of noise such as whispering, background noise, and quantization. 
