\chapter{Evaluation of the Best Classification Method}
\label{chapter_classification}
\section{Considered classification methods}
\label{sec_considered_classif}
In our analysis we implement the following statistical learning methods to predict the class of $ y \in \left\lbrace female, male \right\rbrace$.
\begin{itemize}
	\item \textbf{Logistic Regression:} models the posterior probability of response $y\in \left\lbrace female, male \right\rbrace$, given the predictors $\mat{X}$, using the logistic function;
	
	\item \textbf{Regularized Logistic Regression:} The $\ell_1$-penalty~(LASSO) can be used for variable selection and shrinkage with logistic regression. It is a useful approach to examine which features are  important in voice analysis;
	
	\item \textbf{Linear Discriminant Analysis (LDA):} LDA makes the assumption that the conditional densities $f \left(\mat{X}|y=male \right)$ and $ f \left(\mat{X}|y=female \right)$ are both multivariate Gaussian with a common covariance matrix. It belongs to a family of techniques that use linear boundaries to separate classes in classification problems. If the assumption of normality is realistic, then LDA is expected to provide better results than logistic regression;
	
	\item \textbf{Quadratic Discriminant Analysis (QDA):} QDA is a similar method to LDA. The multivariate normality assumption remains, but unlike LDA, QDA assumes that each class has its own covariance matrix. If QDA has better predictive performance than LDA, it is an indication that a linear boundary is not the optimal to separate the 2 classes.
	
	\item \textbf{k-Nearest Neighbors (kNN):} kNN is a non-parametric approach which is highly flexible and uses non-linear decision boundaries. Before the implementation of kNN, it is crucial to scale the data, since this method relies on the euclidean distance between observations;
	
	\item \textbf{Classifications Trees:} Classification trees stratify the feature space recursively into simple regions and assign the label female or male using the majority vote. Bagging, random forests and gradient tree boosting are also implemented with the aim of improving predictive performance;
	
	\item \textbf{Support Vector Machines (SVM):} SVM perform well in classification problems where there is a clear margin of separation. We experiment with 2 kernels: the linear and the radial basis function~(RBF).
\end{itemize}
\section{Classification based on the fundamental frequency}
\label{sec_intuitive_approach}
Based on the exploratory data analysis described in Chapter~\ref{chapter_data_exploration}, we propose to perform the classification based on the ``meanfun'' feature alone. 
This will give a baseline for further analysis described in the remaining of this Chapter.

To perform the analysis, we randomly split the dataset into a training set~( \SI{80}{\percent}) and a test set~(\SI{20}{\percent}). The training set is used to fit the models and for best parameter selection if needed. The test set is used to compute the classification error and to compare the models. The classification error considered in the study is the $0-1$ loss. 
The experiments have been performed on Python \num{3}\footnote{\url{{https://github.com/AdriBesson/Statistical_learning_course/tree/develop/project}}} with a single seed number for reproducibility of the results.
\begin{table}[htb]
	\ra{1.2}
	\caption{Classification Error of the Methods for Classification With ``meanfun'' Feature}
	\begin{center}
		\begin{tabular}{@{} c c c @{}}\toprule
			Type & Methods & \\
			\midrule
			\multirow{5}{*}{Max. Likelihood} & Logistic reg. & \num{0.0536} \\
			& Logistic reg. - Ridge & \num{0.0505}  \\
			& LDA & \textbf{\num{0.0489}} \\
			& QDA & \textbf{\num{0.0489}} \\
			\cmidrule{1-3}
			\multirow{3}{*}{Trees} & Tree & \num{0.0505} \\
			& Bagging & \num{0.0505} \\
			& XGBoost & \num{0.0505}\\
			\cmidrule{1-3}
			\multirow{2}{*}{SVM} & Linear & \num{0.0505} \\
			& Gaussian & \textbf{\num{0.0489}} \\
			\cmidrule{1-3}
			x & kNN & \num{0.0520}\\
			\bottomrule
		\end{tabular}
	\end{center}
	\label{tab_res_meanfun}
\end{table}

The results, summarized in Table~\ref{tab_res_meanfun}, show that the classification error is already very low when considering only the ``meafun'' feature. Indeed the average classsification error of the different classifiers is \num{0.0509}. Thus, ``meafun'' is a very good feature for classification which is in accordance with the exploratory data analysis described in Chapter~\ref{chapter_data_exploration}. 

Regarding the classifiers, it can be seen that some of the ones described in Section~PUTREF, are not mentioned in Tabel~\ref{tab_res_meanfun}. Indeed, they do not make sense when considering only one feature for classification. 
About the relative performance of the different classifiers, the results are homogeneous around the average classification error and all the classifiers perform well. Zero-order methods such as tree-based methods (with one predictor, tree-based methods are nothing else than a threshold) already give a low classification error. More sophisticated methods, such as linear methods or kernel SVM, give results very similar to or slightly better than zero-order methods.

\section{Classification based on a $80/20$ split of the dataset}
\label{sec_naive_strat}
\subsection{Description}
\subsection{Results}
\begin{table}[htb]
	\ra{1.2}
	\caption{Classification Error of the Methods for Different Seed Numbers With $80/20$ Split}
	\begin{center}
		\begin{tabular}{@{} c c c  c c c c @{}}\toprule
			\multirow{2}{*}{Type} & \multirow{2}{*}{Methods} &  \multicolumn{5}{c}{Seed number}\\
			& & 1 & 2 & 3 & 4 & 5 \\
			\midrule
			\multirow{5}{*}{Max. Likelihood} & Logistic reg. & \num{0.0158} & \num{0.0347} & \num{0.0315} & \num{0.0237} & \num{0.0221} \\
			& Logistic reg. - Ridge & \num{0.0158} & \num{0.0315} & \num{0.0315} & \num{0.0189} & \num{0.0284} \\
			& Logistic reg. - Lasso & \num{0.0315} & \num{0.0363} & \num{0.0379} & \num{0.0284} & \num{0.0300} \\
			& LDA & \num{0.0315} & \num{0.0410} & \num{0.0379} & \num{0.0284} & \num{0.0268} \\
			& QDA & \num{0.0347} & \num{0.0347} & \num{0.0347} & \num{0.0268} & \num{0.0363} \\
			\cmidrule{1-7}
			\multirow{5}{*}{Trees} & Tree & \num{0.0379} & \num{0.0426} & \num{0.0315} & \num{0.0284} & \num{0.0300}\\
			& Pruned Tree & \num{0.0394} & \num{0.0473} & \num{0.0347} & \num{0.0363} & \num{0.0300}\\  
			& Bagging & \num{0.0237} & \num{0.0410} & \num{0.0142} & \textbf{\num{0.0174}} & \num{0.0284}\\
			& Random Forest & \num{0.0189} & \num{0.0347} & \textbf{\num{0.0126}} & \num{0.0205} & \num{0.0205}\\
			& XGBoost & \num{0.0189} & \textbf{\num{0.0268}} & \textbf{\num{0.0126}} & \num{0.0205} & \textbf{\num{0.0189}}\\
			\cmidrule{1-7}
			\multirow{2}{*}{SVM} & Linear & \textbf{\num{0.0142}} & \num{0.0315} & \num{0.0284} & \num{0.0189} & \num{0.0252}\\
			& Gaussian & \num{0.0205} & \num{0.0284} & \num{0.0126} & \num{0.0189} & \num{0.0221}\\
			\cmidrule{1-7}
			x & kNN & \num{0.0300} & \num{0.0347} & \num{0.0252} & \num{0.0379} & \num{0.0300}\\
			\bottomrule
			\end{tabular}
			\end{center}
			\label{tab_res_naive}
\end{table}

\section{Classification based on a $50/50$ split of the dataset}
\begin{table}[htb]
	\ra{1.2}
	\caption{Classification Error of the Methods for Different Seed Numbers With $50/50$ Split}
	\begin{center}
		\begin{tabular}{@{} c c c  c c c c @{}}\toprule
			\multirow{2}{*}{Type} & \multirow{2}{*}{Methods} &  \multicolumn{5}{c}{Seed number}\\
			& & 1 & 2 & 3 & 4 & 5 \\
			\midrule
			\multirow{5}{*}{Max. Likelihood} & Logistic reg. & \num{0.0221} & \num{0.0262} & \num{0.0310} & \num{0.0261} & \num{0.0291} \\
			& Logistic reg. - Ridge & \num{0.0196} & \num{0.0214} & \num{0.0305} & \num{0.0248} & \num{0.0278} \\
			& Logistic reg. - Lasso & \num{0.0284} & \num{0.0294} & \num{0.0368} & \num{0.0348} & \num{0.0364} \\
			& LDA & \num{0.0291} & \num{0.0269} & \num{0.0342} & \num{0.0309} & \num{0.0313} \\
			& QDA & \num{0.0280} & \num{0.0257} & \num{0.0370} & \num{0.0362} & \num{0.0365} \\
			\cmidrule{1-7}
			\multirow{5}{*}{Trees} & Tree & \num{0.0284} & \num{0.0317} & \num{0.0387} & \num{0.0454} & \num{0.0468}\\
			& Pruned Tree & \num{0.0394} & \num{0.0473} & \num{0.0347} & \num{0.0363} & \num{0.0300}\\  
			& Bagging & \num{0.0209} & \num{0.0244} & \num{0.0319} & \num{0.0253} & \num{0.0302}\\
			& Random Forest & \num{0.0191} & \num{0.0206} & \num{0.0305} & \num{0.0248} & \num{0.0290}\\
			& XGBoost & \textbf{\num{0.0170}} & \textbf{\num{0.0149}} & \textbf{\num{0.0259}} & \textbf{\num{0.0197}} & \textbf{\num{0.0227}}\\
			\cmidrule{1-7}
			\multirow{2}{*}{SVM} & Linear & \num{0.0209} & \num{0.0149} & \num{0.0259} & \num{0.0197} & \num{0.0227}\\
			& Gaussian & \num{0.0184} & \num{0.0188} & \num{0.0272} & \num{0.0211} & \num{0.0234}\\
			\cmidrule{1-7}
			x & kNN & \num{0.0273} & \num{0.0312} & \num{0.0326} & \num{0.0410} & \num{0.0382}\\
			\bottomrule
		\end{tabular}
	\end{center}
	\label{tab_res_our_strategy}
\end{table}