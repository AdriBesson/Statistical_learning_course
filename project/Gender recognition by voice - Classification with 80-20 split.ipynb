{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Gender recognition by voice\n",
    "## EPFL - Statistical learning (MATH-412) \n",
    "## Adrien Besson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Classification with 80/20 split of the dataset and different seed numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.discriminant_analysis as lda\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn import svm, tree, ensemble, neighbors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "input_file = os.path.join(os.getcwd(), 'data', 'voice.csv')\n",
    "data = pd.read_csv(input_file)\n",
    "data['label'] = data['label'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop collinear columns\n",
    "cols_to_drop = ['IQR', 'dfrange', 'centroid']\n",
    "data = data.drop(cols_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assign quantitative values to the labels and drop them from the data\n",
    "encoder = preproc.LabelEncoder()\n",
    "labels = data['label'].values\n",
    "labels = encoder.fit_transform(labels)\n",
    "data = data.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale the features to have mean 0 and variance 1\n",
    "features = preproc.scale(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification based on all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "seed = 2\n",
    "features = data.values\n",
    "features_train, features_test, labels_train, labels_test = model_selection.train_test_split(features, labels, train_size=0.8, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss kNN: 26.813880126182966 %\n"
     ]
    }
   ],
   "source": [
    "# k-NN classification\n",
    "param_grid = [\n",
    "  {'n_neighbors': np.arange(1, 20, 1), 'weights':['distance', 'uniform']}\n",
    "] \n",
    "knn_grid = model_selection.GridSearchCV(estimator=neighbors.KNeighborsClassifier(), cv=10, param_grid=param_grid, n_jobs=-1)\n",
    "knn_grid.fit(X=features_train, y=labels_train)\n",
    "class_score_knn = knn_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss kNN: {0} %'.format((1-class_score_knn)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA/QDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - LDA: 3.785488958990535 %\n"
     ]
    }
   ],
   "source": [
    "lda_class = lda.LinearDiscriminantAnalysis()\n",
    "lda_class.fit(X=features_train, y=labels_train)\n",
    "class_score_lda = lda_class.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - LDA: {0} %'.format((1-class_score_lda)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - QDA: 4.57413249211357 %\n"
     ]
    }
   ],
   "source": [
    "qda_class = lda.QuadraticDiscriminantAnalysis()\n",
    "qda_class.fit(X=features_train, y=labels_train)\n",
    "class_score_qda = qda_class.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - QDA: {0} %'.format((1-class_score_qda)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Logistic : 2.8391167192429068 %\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression\n",
    "logistic = lm.LogisticRegression(penalty='l2', solver='liblinear', random_state=seed)\n",
    "logistic.set_params(C=1e9)\n",
    "logistic.fit(features_train, labels_train)\n",
    "score = logistic.score(features_test, labels_test)\n",
    "logistic.fit(X=features_train, y=labels_train)\n",
    "class_score_l2 = logistic.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Logistic : {0} %'.format((1-class_score_l2)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression - Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Logistic Ridge: 2.8391167192429068 %\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression with L2 regularization\n",
    "logistic_reg = lm.LogisticRegressionCV(penalty='l2', solver='liblinear', random_state=seed)\n",
    "logistic_reg.fit(X=features_train, y=labels_train)\n",
    "class_score_reg_l2 = logistic_reg.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Logistic Ridge: {0} %'.format((1-class_score_reg_l2)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression - LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification loss - Logistic LASSO: 2.681388012618302\n"
     ]
    }
   ],
   "source": [
    "logistic_reg_l1 = lm.LogisticRegressionCV(penalty='l1', solver='liblinear', random_state=seed)\n",
    "logistic_reg_l1.fit(X=features_train, y=labels_train)\n",
    "class_score_reg_l1 = logistic_reg_l1.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Logistic LASSO: {0}'.format((1-class_score_reg_l1)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM classification - Linear kernel\n",
    "param_grid = [\n",
    "  {'C': [8, 9, 10 ,11 ,12], 'gamma':[0.2, 0.21, 0.22, 0.23, 0.24, 0.25]}\n",
    "]\n",
    "class_svm = svm.SVC(kernel='linear', random_state=seed)\n",
    "class_svm_grid = model_selection.GridSearchCV(estimator=class_svm, cv=10, param_grid=param_grid, n_jobs=-1)\n",
    "class_svm_grid.fit(X=features_train, y=labels_train)\n",
    "score_svm = class_svm_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - linear SVM: {0} %'.format((1-score_svm)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel SVM - RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM classification - RBF kernel\n",
    "param_grid = [\n",
    "  {'C': 2.0**(np.arange(-5, 15, 2)), 'gamma':[0.2, 0.21, 0.22, 0.23, 0.24, 0.25], 'kernel':['rbf']}\n",
    "]\n",
    "class_svm = svm.SVC(random_state=seed)\n",
    "class_svm_grid = model_selection.GridSearchCV(estimator=class_svm, cv=10, param_grid=param_grid, n_jobs=-1)\n",
    "class_svm_grid.fit(X=features_train, y=labels_train)\n",
    "score_svm = class_svm_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - kernel SVM - L2: {0} %'.format((1-score_svm)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'min_samples_leaf': [1, 5,10,50,100,200,500]}\n",
    "]\n",
    "dec_tree = tree.DecisionTreeClassifier(criterion='entropy', random_state=seed)\n",
    "dec_tree_grid = model_selection.GridSearchCV(estimator=dec_tree, cv=10, param_grid=param_grid, n_jobs=-1)\n",
    "dec_tree_grid.fit(X=features_train, y=labels_train)\n",
    "score_tree = dec_tree_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Decision tree: {0} %'.format((1-score_tree)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'min_samples_leaf': [1, 5,10,50,100,200,500]}\n",
    "]\n",
    "random_forest = ensemble.RandomForestClassifier(criterion='entropy', random_state=seed, n_jobs=-1)\n",
    "random_forest_grid = model_selection.GridSearchCV(estimator=random_forest, cv=10, param_grid=param_grid, n_jobs=-1)\n",
    "random_forest_grid.fit(X=features_train, y=labels_train)\n",
    "score_rf = random_forest_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Random forest: {0} %'.format((1-score_rf)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'n_estimators': [1, 5,10,50,100,200,500], 'learning_rate':[1e-3, 1e-2, 1e-1, 1, 10]}\n",
    "]\n",
    "ada_boost = ensemble.AdaBoostClassifier(random_state=seed)\n",
    "ada_boost_grid = model_selection.GridSearchCV(estimator=ada_boost, cv=10, param_grid=param_grid)\n",
    "ada_boost_grid.fit(X=features_train, y=labels_train)\n",
    "score_ab = ada_boost_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - AdaBoost: {0} %'.format((1-score_ab)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'n_estimators': [1, 5,10,50,100,200,500], 'learning_rate':[1e-3, 1e-2, 1e-1, 1, 10], 'min_samples_leaf': [1, 5,10,50,100,200,500]}\n",
    "]\n",
    "g_boost = ensemble.GradientBoostingClassifier(random_state=seed)\n",
    "g_boost_grid = model_selection.GridSearchCV(estimator=g_boost, cv=10, param_grid=param_grid)\n",
    "g_boost_grid.fit(X=features_train, y=labels_train)\n",
    "score_gb = g_boost_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Gradient Boosting: {0} %'.format((1-score_gb)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'n_estimators': [1, 5,10,50,100,200,500]}\n",
    "]\n",
    "bagging = ensemble.BaggingClassifier(random_state=seed)\n",
    "bagging_grid = model_selection.GridSearchCV(estimator=bagging, cv=10, param_grid=param_grid)\n",
    "bagging_grid.fit(X=features_train, y=labels_train)\n",
    "score_bag = bagging_grid.score(X=features_test, y=labels_test)\n",
    "print('Classification loss - Bagging: {0} %'.format((1-score_bag)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    " 1. Tree-based methods seem to give the best results but the results are unstable with respect to the shift of the training and test set (To see that, just run the script with a different random_state for the split)\n",
    " 1. The average classification error is significantly decreased compared to the case where only meanfun was used.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
